# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import io
import logging
import os
import random
import re

import requests
# from openai import OpenAI
from PIL import Image

import verl.utils.torch_functional as verl_F
from verl.utils.dataset.rl_dataset import RLHFDataset
from verl.utils.model import compute_position_id_with_mask

logger = logging.getLogger(__name__)

# openai_api_key = "EMPTY"
# openai_api_base = os.environ.get("LLM_AS_A_JUDGE_BASE", "http://10.1.100.71:18901/v1")

# client = OpenAI(
#     api_key=openai_api_key,
#     base_url=openai_api_base,
# )

# model_name = ""
# if openai_api_base:
#     try:
#         response = requests.get(f"{openai_api_base}/models")
#         response.raise_for_status()
#         models = response.json()
#         if models.get("data"):
#             model_name = models["data"][0]["id"]
#         else:
#             logger.warning("No models found at the specified API base for reward scoring.")
#     except (requests.exceptions.RequestException, KeyError, IndexError) as e:
#         logger.warning(f"Failed to get model from {openai_api_base}: {e}. Reward scoring will be disabled.")


class CustomRLHFDataset(RLHFDataset):
    # prompt_key: 'prompt' by default
    # image_key: 'images' by default
    def __getitem__(self, item):
        """
        Note that we also return the raw_input_ids so that it can be combined with other chat template
        """
        row_dict: dict = self.dataframe[item]
        row_dict[self.prompt_key] = [
            {
                "role": "system",
                # We don't need tool description, because custom_chat_template will add it.
                "content": (
                    "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚ä½ å¯ä»¥è°ƒç”¨å‡½æ•°æ¥ååŠ©å¤„ç†ç”¨æˆ·çš„è¯·æ±‚ã€‚"
                    "é‡è¦ï¼šä½ ä¸€æ¬¡åªèƒ½è°ƒç”¨ä¸€ä¸ªå‡½æ•°ã€‚"
                    "æ¯æ¬¡è°ƒç”¨å‡½æ•°åï¼Œå¦‚æœéœ€è¦ç»§ç»­è°ƒç”¨ä¸‹ä¸€ä¸ªå‡½æ•°ï¼Œè¯·å…ˆç­‰å¾…è¯¥å‡½æ•°çš„æ‰§è¡Œç»“æœã€‚"
                ),
            },
            {
                "role": "user",
                "content": row_dict[self.prompt_key][1]["content"],
            },
        ]
        messages = self._build_messages(row_dict)
        model_inputs = {}

        if self.processor is not None:
            raw_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            multi_modal_data = {}

            images = None
            if self.image_key in row_dict and row_dict.get(self.image_key, None) is not None:
                # images = [Image.open(io.BytesIO(image["bytes"])) for image in row_dict.pop(self.image_key)]
                images = [image for image in row_dict.pop(self.image_key)]
                
                # logger.warning(f"Number of images: {len(images)}")

                # due to the image key is "image" instead of "images" in vllm, we need to use "image" here
                # link: https://github.com/vllm-project/vllm/blob/3c545c0c3b98ee642373a308197d750d0e449403/vllm/multimodal/parse.py#L205  # noqa: E501
                multi_modal_data["image"] = images

            model_inputs = self.processor(text=[raw_prompt], images=images, return_tensors="pt")

            input_ids = model_inputs.pop("input_ids")
            attention_mask = model_inputs.pop("attention_mask")

            if "second_per_grid_ts" in model_inputs:
                model_inputs.pop("second_per_grid_ts")

            # There's a trap here, multi_modal_inputs has to be a dict, not BatchFeature
            row_dict["multi_modal_data"] = multi_modal_data

            # We will do batch.union() in the trainer,
            # so we cannot have "multi_modal_inputs" in row_dict if rollout generates new multi_modal_inputs
            if self.return_multi_modal_inputs:
                row_dict["multi_modal_inputs"] = dict(model_inputs)

                # second_per_grid_ts isn't used for training, just for mrope
                row_dict["multi_modal_inputs"].pop("second_per_grid_ts", None)

        else:
            raw_prompt = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            model_inputs = self.tokenizer(raw_prompt, return_tensors="pt", add_special_tokens=False)
            input_ids = model_inputs.pop("input_ids")
            attention_mask = model_inputs.pop("attention_mask")

        input_ids, attention_mask = verl_F.postprocess_data(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=self.max_prompt_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=True,
            truncation=self.truncation,
        )

        if self.processor is not None and "Qwen2VLImageProcessor" in self.processor.image_processor.__class__.__name__:
            from verl.models.transformers.qwen2_vl import get_rope_index

            position_ids = [
                get_rope_index(
                    self.processor,
                    input_ids=input_ids[0],
                    image_grid_thw=model_inputs.get("image_grid_thw"),
                    video_grid_thw=model_inputs.get("video_grid_thw"),
                    second_per_grid_ts=model_inputs.get("second_per_grid_ts"),
                    attention_mask=attention_mask[0],
                )
            ]  # (1, 3, seq_len)

        else:
            position_ids = compute_position_id_with_mask(attention_mask)

        row_dict["input_ids"] = input_ids[0]
        row_dict["attention_mask"] = attention_mask[0]
        row_dict["position_ids"] = position_ids[0]

        raw_prompt_ids = self.tokenizer.encode(raw_prompt, add_special_tokens=False)
        if len(raw_prompt_ids) > self.max_prompt_length:
            if self.truncation == "left":
                raw_prompt_ids = raw_prompt_ids[-self.max_prompt_length :]
            elif self.truncation == "right":
                raw_prompt_ids = raw_prompt_ids[: self.max_prompt_length]
            elif self.truncation == "middle":
                left_half = self.max_prompt_length // 2
                right_half = self.max_prompt_length - left_half
                raw_prompt_ids = raw_prompt_ids[:left_half] + raw_prompt_ids[-right_half:]
            elif self.truncation == "error":
                raise RuntimeError(f"Prompt length {len(raw_prompt_ids)} is longer than {self.max_prompt_length}.")

        row_dict["raw_prompt_ids"] = raw_prompt_ids
        # encode prompts without chat template
        if self.return_raw_chat:
            row_dict["raw_prompt"] = messages

        # get prompts with chat template
        if self.return_full_prompt:
            row_dict["full_prompts"] = raw_prompt  # array of strings

        # add index for each prompt
        index = row_dict.get("extra_info", {}).get("index", 0)
        tools_kwargs = {
            "image_zoom_in_tool": {
                "create_kwargs": {"image": images[0]},
                # "execute_kwargs": {},
                # "calc_reward_kwargs": {},
                # "release_kwargs": {},
            }
        }
        row_dict["index"] = index
        row_dict["tools_kwargs"] = tools_kwargs
        row_dict["agent_name"] = "tool_agent"
        return row_dict


# if "LLM_AS_A_JUDGE_BASE" not in os.environ:
#         try:
#             with open("./tmp/llm_ip.txt") as f:
#                 os.environ["LLM_AS_A_JUDGE_BASE"] = f.read().strip()
#                 print("[Auto Set] LLM_AS_A_JUDGE_BASE from file:", os.environ["LLM_AS_A_JUDGE_BASE"])
#         except Exception as e:
#             print("[Error] Could not load LLM_AS_A_JUDGE_BASE from file:", str(e))

# OPENAI_API_KEY = "EMPTY"
# OPENAI_API_BASE = os.environ.get("LLM_AS_A_JUDGE_BASE", None)
# if not OPENAI_API_BASE:
#     raise ValueError("[DEBUG] LLM_AS_A_JUDGE_BASE environment variable is not set.")
# MODEL_NAME = requests.get(f"{OPENAI_API_BASE}/models").json()['data'][0]['id']
# CLIENT = OpenAI(
#         api_key=OPENAI_API_KEY,
#         base_url=OPENAI_API_BASE,
#     )

import re

def check_model_format(predict_str):
    """
    æ£€æŸ¥æ¨¡å‹å›å¤æ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼š
    1. æ¨¡å‹å›å¤å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ <think>...</think><tool_call>...</tool_call> æˆ– <think>...</think><answer>...</answer>
    2. <think> å†…éƒ¨ä¸å¾—åµŒå¥— <tool_call> æˆ– <answer>
    3. <tool_call> å’Œ <answer> ä¸å¾—åŒæ—¶å‡ºç°
    4. åªæœ‰æœ€åä¸€ä¸ªæ¨¡å‹å›å¤å¯ä»¥å‡ºç° <answer>
    5. æ¯ä¸ªæ¨¡å‹å›å¤å¿…é¡»åŒ…å«ä¸”ä»…åŒ…å«ä¸€ä¸ª <think>...</think>
    6. <think> å¿…é¡»åœ¨ <tool_call> æˆ– <answer> ä¹‹å‰
    7. æ¯ä¸ªæ¨¡å‹å›å¤å†…ä¸å…è®¸å‡ºç°å¤šä¸ª <think>/<tool_call>/<answer>
    8. æ‰€æœ‰æ¨¡å‹å›å¤ä¸­è‡³å°‘è¦æœ‰ä¸€ä¸ª <answer>
    """
    is_format_error = False
    error_messages = []

    # --- Step 1: åˆ†ç¦»ç³»ç»Ÿå›å¤å’Œæ¨¡å‹å›å¤ ---
    # system_blocks = re.findall(
    #     r"<\|im_start\|>\s*user\s*<tool_response>.*?</tool_response>\s*<\|im_end\|>",
    #     predict_str, flags=re.DOTALL)
    model_blocks = re.split( r"\s*user\s*<tool_response>.*?</tool_response>\s*", predict_str, flags=re.DOTALL)
    model_blocks = [block.strip() for block in model_blocks if block.strip()]

    # print("System blocks:", len(system_blocks))
    # print("Model blocks:", len(model_blocks))

    total_answer_count = 0  # æ‰€æœ‰æ¨¡å‹å›å¤çš„ <answer> å‡ºç°æ¬¡æ•°

    for idx, block in enumerate(model_blocks):
        # think_count = block.count("<think>")
        tool_call_count = block.count("<tool_call>")
        answer_count = block.count("<answer>")
        total_answer_count += answer_count

        # è§„åˆ™æ£€æŸ¥
        # if think_count == 0:
            # is_format_error = True
            # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: ç¼ºå°‘ <think> æ ‡ç­¾")
        # elif think_count > 1:
            # is_format_error = True
            # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: å‡ºç°äº†å¤šä¸ª <think> æ ‡ç­¾")
        if tool_call_count > 1:
            is_format_error = True
            error_messages.append(f"æ¨¡å‹å›å¤ {idx}: å‡ºç°äº†å¤šä¸ª <tool_call> æ ‡ç­¾")
        if answer_count > 1:
            is_format_error = True
            error_messages.append(f"æ¨¡å‹å›å¤ {idx}: å‡ºç°äº†å¤šä¸ª <answer> æ ‡ç­¾")

        # if block.count("<think>") != block.count("</think>"):
            # is_format_error = True
            # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: <think> æ ‡ç­¾ä¸åŒ¹é…")

        # think_contents = re.findall(r"<think>(.*?)</think>", block, flags=re.DOTALL)
        # for content in think_contents:
            # if "<tool_call>" in content or "<answer>" in content:
                # is_format_error = True
                # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: <think> å†…åµŒå¥—äº†éæ³•æ ‡ç­¾")

        if tool_call_count > 0 and answer_count > 0:
            is_format_error = True
            error_messages.append(f"æ¨¡å‹å›å¤ {idx}: åŒæ—¶å­˜åœ¨ <tool_call> å’Œ <answer>")

        if idx < len(model_blocks) - 1 and answer_count > 0:
            is_format_error = True
            error_messages.append(f"æ¨¡å‹å›å¤ {idx}: éæ³•å‡ºç° <answer>ï¼ˆåªèƒ½åœ¨æœ€åä¸€ä¸ªæ¨¡å‹å›å¤å‡ºç°ï¼‰")

        # <think> å¿…é¡»åœ¨ <tool_call> æˆ– <answer> ä¹‹å‰
        # think_pos = block.find("<think>")
        # if tool_call_count > 0 and think_pos > block.find("<tool_call>"):
            # is_format_error = True
            # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: <think> å‡ºç°åœ¨ <tool_call> ä¹‹å")
        # if answer_count > 0 and think_pos > block.find("<answer>"):
            # is_format_error = True
            # error_messages.append(f"æ¨¡å‹å›å¤ {idx}: <think> å‡ºç°åœ¨ <answer> ä¹‹å")

    # å…¨å±€æ£€æŸ¥
    if total_answer_count == 0:
        is_format_error = True
        error_messages.append("æ‰€æœ‰æ¨¡å‹å›å¤éƒ½ç¼ºå°‘ <answer> æ ‡ç­¾")

    return is_format_error, error_messages



def extract_fields(model_output: str):
    result = {}

    # æ‰€æœ‰è¯„åˆ†å­—æ®µ
    fields = ["ç»“æ„", "æœä»£", "çš‡å¸", "é‡‰è‰²", "çº¹é¥°", "å™¨å‹"]
    for field in fields:
        match = re.search(rf'"?{field}"?\s*:\s*(-?\d+)', model_output)
        result[field] = int(match.group(1)) if match else None

    # # ç†ç”±å­—æ®µæå–ï¼Œå…è®¸å¤šè¡Œã€éè´ªå©ªåŒ¹é…ï¼Œç»“å°¾å¯æ— æ ‡ç‚¹
    # match_reason = re.search(
    #   r'"?ç†ç”±"?\s*:\s*"([\s\S]*?)"\s*$', model_output.strip()
    # )
    # result["ç†ç”±"] = match_reason.group(1).strip() if match_reason else ""

    return result

# def get_chat_template():
#     chat_template_V1 = """
# ---

# ä½ æ˜¯ä¸€ä½ä¸­å›½å¤é™¶ç“·é¢†åŸŸçš„ä¸“å®¶è¯„å®¡å‘˜ï¼Œä¸“é—¨è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„ç“·å™¨é‰´å®šæ–‡æœ¬è´¨é‡ã€‚

# è¯·æ ¹æ®ä»¥ä¸‹â€œå‚è€ƒç­”æ¡ˆâ€ä¸â€œæ¨¡å‹è¾“å‡ºâ€ï¼Œæ ¹æ®å¦‚ä¸‹è§„åˆ™ä»6ä¸ªç»´åº¦è¿›è¡Œå…¨é¢è¯„åˆ†ï¼Œæœ€ååªè¾“å‡ºå¯¹åº”çš„è¯„åˆ†ã€‚

# ---

# ### ğŸ§¾ è¯„åˆ†è¦æ±‚ï¼š

# è¯·åˆ†åˆ«ä»ä»¥ä¸‹ 6 ä¸ªç»´åº¦ï¼Œé€é¡¹ç»™å‡º 1ï½5 åˆ†è¯„åˆ†ï¼Œè¦æ±‚è¯„åˆ†é£æ ¼åä¿å®ˆã€‚

# ### ğŸ” å…·ä½“è¯„åˆ†è§„åˆ™ï¼š

# 1. **å™¨å‹æè¿°å‡†ç¡®æ€§**  
#    æ˜¯å¦åˆç†è¯†åˆ«å¹¶æè¿°äº†å™¨å‹ï¼›æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆè¡¨è¿°ä¸€è‡´ï¼Œæˆ–æœªè¡¨è¾¾/è¡¨è¾¾é”™è¯¯ã€‚

# 2. **é‡‰è‰²æè¿°å‡†ç¡®æ€§**  
#    æ˜¯å¦æåŠå¹¶æ­£ç¡®è¡¨è¿°é‡‰è‰²ç‰¹å¾ï¼ˆå¦‚é’èŠ±ã€ç²‰å½©ã€çº¢åœ°ç»¿å½©ç­‰ï¼‰ï¼›æ˜¯å¦åç¦»æˆ–ç¼ºå¤±ã€‚

# 3. **çº¹é¥°å†…å®¹å‡†ç¡®æ€§**  
#    æ˜¯å¦æ­£ç¡®æè¿°è£…é¥°å›¾æ¡ˆã€é¢˜æï¼ˆå¦‚é¾™å‡¤ã€èŠ±å‰ã€äººç‰©ã€äº‘çº¹ç­‰ï¼‰ï¼›æ˜¯å¦ä¸æ—¶ä»£é£æ ¼åŒ¹é…ã€‚

# 4. **æœä»£åˆ¤æ–­å‡†ç¡®æ€§**  
#    åªè¦æ¨¡å‹æ­£ç¡®è¯†åˆ«å‡ºâ€œå”â€ã€â€œå®‹â€ã€â€œå…ƒâ€ã€â€œæ˜â€ã€â€œæ¸…â€ç­‰å¤§æœä»£ï¼Œå³è§†ä¸ºå‡†ç¡®ï¼Œå¯å¾—æ»¡åˆ†ã€‚æ— é¡»åˆ¤æ–­å…·ä½“çš‡å¸ã€‚

# 5. **çš‡å¸åˆ¤æ–­å‡†ç¡®æ€§**
#    ï¼ˆ1ï¼‰å¦‚æœæœä»£åˆ¤æ–­é”™äº†ï¼Œåˆ™ç»™0åˆ†ã€‚
#    ï¼ˆ2ï¼‰åˆ¤æ–­â€œå‚è€ƒç­”æ¡ˆâ€ä¸­æ˜¯å¦åŒ…å«äº†çš‡å¸åç§°ï¼Œå¦‚â€œæ˜å˜‰é–â€æˆ–â€œæ¸…é›æ­£â€ç­‰ï¼Œä¸åŒ…å«çš„è¯ç»™-1åˆ†ï¼ˆå¦‚â€œæ¸… é’èŠ±ç“·å©´æˆå›¾ç¢—â€ï¼‰ã€‚å¦‚æœâ€œå‚è€ƒç­”æ¡ˆâ€åŒ…å«äº†çš‡å¸åç§°ï¼Œå†åˆ¤æ–­â€œæ¨¡å‹è¾“å‡ºâ€æ˜¯å¦åŒ…å«äº†ç›¸åŒçš„çš‡å¸ï¼Œå¦‚æœç›¸åŒåˆ™ç»™5åˆ†ï¼Œå¦‚æœå†™é”™çš‡å¸ï¼Œç»™0åˆ†ã€‚

# ---

# ### âœ… è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼Œä¸è¦è¿”å›ç†ç”±ï¼š

# "å™¨å‹": X,
# "é‡‰è‰²": X,
# "çº¹é¥°": X,
# "æœä»£": X,
# "çš‡å¸": X

# ---

# ### âœ… ç¤ºä¾‹æ ¼å¼ï¼ˆä¾›æ¨¡å‹å‚è€ƒï¼‰ï¼š

# "å™¨å‹": 5,
# "é‡‰è‰²": 4,
# "çº¹é¥°": 3,
# "æœä»£": 5,
# "çš‡å¸": -1
# ---
# ä»¥ä¸‹æ˜¯å‚è€ƒç­”æ¡ˆå’Œæ¨¡å‹è¾“å‡ºï¼š
# [å‚è€ƒç­”æ¡ˆ]ï¼š{}
# [æ¨¡å‹è¾“å‡º]ï¼š{}
# ä½ çš„è¯„åˆ†æ˜¯
# """
#     chat_template_V2 = """
# ---

# ä½ æ˜¯ä¸€ä½ä¸­å›½å¤é™¶ç“·é¢†åŸŸçš„ä¸“å®¶è¯„å®¡å‘˜ï¼Œä¸“é—¨è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„ç“·å™¨é‰´å®šå‡†ç¡®æ€§ã€‚

# è¯·æ ¹æ®ä»¥ä¸‹â€œå‚è€ƒç­”æ¡ˆâ€ä¸â€œæ¨¡å‹è¾“å‡ºâ€ï¼ŒæŒ‰ç…§5ä¸ªç»´åº¦ç»™åˆ†ï¼Œæ¯ä¸ªç»´åº¦è¯„åˆ†ç»“æœæ§åˆ¶åœ¨0-1ä¹‹é—´ã€‚

# ---

# ### ğŸ§¾ è¯„åˆ†è¦æ±‚ï¼š

# 1. **é‰´å®šç»“æœç»“æ„æ€§**
#     æ˜¯å¦ä¸¥æ ¼æŒ‰ç…§â€œæœä»£çš‡å¸ é‡‰è‰²çº¹é¥°å™¨å‹â€æˆ–è€…â€œæœä»£ é‡‰è‰²çº¹é¥°å™¨å‹â€çš„æ ¼å¼ç»™å‡ºç“·å™¨çš„é‰´å®šç»“æœã€‚
#     å¦‚æœæ ¼å¼æ­£ç¡®ï¼Œç»™1åˆ†ï¼›å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œç»™0åˆ†ã€‚

# 2. **æœä»£åˆ¤æ–­å‡†ç¡®æ€§**  
#    åªè¦æ¨¡å‹æ­£ç¡®è¯†åˆ«å‡ºâ€œå”â€ã€â€œå®‹â€ã€â€œå…ƒâ€ã€â€œæ˜â€ã€â€œæ¸…â€ç­‰å¤§æœä»£ï¼Œå³è§†ä¸ºå‡†ç¡®ï¼Œå¯å¾—æ»¡åˆ†ã€‚æ— é¡»åˆ¤æ–­å…·ä½“çš‡å¸ã€‚
#    å¦‚æœæ¨¡å‹è¾“å‡ºçš„æœä»£ä¸å‚è€ƒç­”æ¡ˆä¸­çš„æœä»£ä¸€è‡´ï¼Œç»™1åˆ†ï¼›å¦‚æœä¸ä¸€è‡´ï¼Œç»™0åˆ†ã€‚

# 3. **çš‡å¸åˆ¤æ–­å‡†ç¡®æ€§**
#    ï¼ˆ1ï¼‰å¦‚æœå‚è€ƒç­”æ¡ˆä¸­åªæœ‰æœä»£æ²¡æœ‰å†™å…·ä½“çš‡å¸å¹´å·ï¼Œç»™-1ã€‚
#    ï¼ˆ2ï¼‰å¦‚æœâ€œå‚è€ƒç­”æ¡ˆâ€å³åŒ…å«æœä»£ï¼ŒåˆåŒ…å«å…·ä½“çš‡å¸å¹´å·ï¼Œå¦‚â€œæ˜å˜‰é–â€ã€â€œæ¸…åº·ç†™â€ç­‰ï¼Œåˆ™åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¾“å‡ºæ­£ç¡®çš„æœä»£ä»¥åŠçš‡å¸å¹´å·ï¼š
#       ï¼ˆ1.1ï¼‰è‹¥æ²¡æœ‰è¾“å‡ºæ­£ç¡®çš„æœä»£æˆ–è€…çš‡å¸å¹´å·ï¼Œå¾—0åˆ†ï¼›
#       ï¼ˆ1.2ï¼‰è‹¥æ­£ç¡®è¾“å‡ºäº†æœä»£ä»¥åŠçš‡å¸å¹´å·ï¼Œå¾—1åˆ†ã€‚

# 4. **é‡‰è‰²æè¿°å‡†ç¡®æ€§**  
#    æ˜¯å¦æåŠå¹¶æ­£ç¡®è¡¨è¿°é‡‰è‰²ç‰¹å¾ï¼ˆå¦‚é’èŠ±ã€ç²‰å½©ã€çº¢åœ°ç»¿å½©ç­‰ï¼‰ï¼›æ˜¯å¦åç¦»æˆ–ç¼ºå¤±ã€‚
#     æ ¹æ®å‡†ç¡®ç¨‹åº¦åœ¨0-1ä¹‹é—´æ‰“åˆ†ã€‚

# 5. **çº¹é¥°å†…å®¹å‡†ç¡®æ€§**  
#    æ˜¯å¦æ­£ç¡®æè¿°è£…é¥°å›¾æ¡ˆã€é¢˜æï¼ˆå¦‚é¾™å‡¤ã€èŠ±å‰ã€äººç‰©ã€äº‘çº¹ç­‰ï¼‰ï¼›æ˜¯å¦ä¸æ—¶ä»£é£æ ¼åŒ¹é…ã€‚
#     æ ¹æ®å‡†ç¡®ç¨‹åº¦åœ¨0-1ä¹‹é—´æ‰“åˆ†ã€‚

# 6. **å™¨å‹æè¿°å‡†ç¡®æ€§**  
#    æ˜¯å¦åˆç†è¯†åˆ«å¹¶æè¿°äº†å™¨å‹ï¼›æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆè¡¨è¿°ä¸€è‡´ï¼Œæˆ–æœªè¡¨è¾¾/è¡¨è¾¾é”™è¯¯ã€‚
#    æ ¹æ®å‡†ç¡®ç¨‹åº¦åœ¨0-1ä¹‹é—´æ‰“åˆ†ã€‚





# ### âœ… è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„æ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼Œä¸è¦è¿”å›ç†ç”±ï¼š

# "ç»“æ„": X,
# "æœä»£": X,
# "çš‡å¸": X,
# "é‡‰è‰²": X,
# "çº¹é¥°": X,
# "å™¨å‹": X,


# ---

# ### âœ… ç¤ºä¾‹æ ¼å¼ï¼ˆä¾›æ¨¡å‹å‚è€ƒï¼‰ï¼š

# "ç»“æ„": 1,
# "æœä»£": 1,
# "çš‡å¸": -1,
# "é‡‰è‰²": 0.8,
# "çº¹é¥°": 0.6,
# "å™¨å‹": 1,

# ---
# ä»¥ä¸‹æ˜¯å‚è€ƒç­”æ¡ˆå’Œæ¨¡å‹è¾“å‡ºï¼š
# [å‚è€ƒç­”æ¡ˆ]ï¼š{}
# [æ¨¡å‹è¾“å‡º]ï¼š{}
# ä½ çš„è¯„åˆ†æ˜¯
# """

#     return chat_template_V2


# def get_prompt(ground_truth, output):
#     prompt = get_chat_template().format(ground_truth, output)
#     return prompt


def extract_answer(text):
    """
    ä»ç»™å®šçš„æ–‡æœ¬ä¸­æå–<answer></answer>æ ‡ç­¾å†…éƒ¨çš„å†…å®¹ã€‚
    
    å‚æ•°:
        text (str): åŒ…å«<answer>æ ‡ç­¾çš„æ–‡æœ¬
        
    è¿”å›:
        str or None: æ ‡ç­¾å†…éƒ¨çš„å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›Noneã€‚
    """
    # ä½¿ç”¨éè´ªå©ªæ¨¡å¼åŒ¹é…<answer>å’Œ</answer>ä¹‹é—´çš„å†…å®¹
    pattern = r'<answer>(.*?)</answer>'
    match = re.search(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None


# def compute_score_V1(predict_str: str, ground_truth: str, extra_info=None) -> float:
#     """
#     Compute the score based on the predicted string and ground truth.

#     In this version, the scoring is adjusted to give more weight to the tool usage and format correctness.
    
#     Args:
#         predict_str (str): The predicted string.
#         ground_truth (str): The ground truth string.
#         extra_info (dict, optional): Additional information, not used in this function.

#     Returns:
#         float: The computed score.
#     """
#     # format_score
#     is_format_error = False
#     count_think_1 = predict_str.count("<think>")
#     count_think_2 = predict_str.count("</think>")
#     if count_think_1 != count_think_2:
#         is_format_error = True

#     count_vision_1 = predict_str.count("<|vision_start|><|image_pad|>")
#     count_vision_2 = predict_str.count("<|image_pad|><|vision_end|>")
#     if count_vision_1 != count_vision_2:
#         is_format_error = True

#     predict_no_think = predict_str.split('</think>')[-1].strip()
#     count_answer_1 = predict_no_think.count("<answer>")
#     count_answer_2 = predict_no_think.count("</answer>")
#     if count_answer_1 != count_answer_2:
#         is_format_error = True
    

#     answer_text = predict_str.split("<answer>")[-1].split("</answer>")[0].strip()
#     if len(answer_text) > 5:
#        acc_reward = 0.0
#     elif ground_truth in answer_text:
#         acc_reward = 1.0
#     else:
#         acc_reward = 0.0

#     tool_reward_1 = 1.0 if count_vision_1 > 0 else 0.0
#     tool_reward_2 = 1.0 if count_vision_1 > 0 and acc_reward > 0.5 else 0.0
#     format_reward = -1.0 if is_format_error else 0.0

#     res = 0.8 * acc_reward + 0.2 * format_reward + 0.5 * tool_reward_1 + 0.5 * tool_reward_2

#     return res


# def compute_score(data_source: str, solution_str: str, ground_truth: str, extra_info=None) -> float:
#     """
#     Compute the score based on the predicted string and ground truth.

#     In this version, the scoring is adjusted to give more weight to the tool usage and format correctness.
    
#     Args:
#         predict_str (str): The predicted string.
#         ground_truth (str): The ground truth string.
#         extra_info (dict, optional): Additional information, not used in this function.

#     Returns:
#         float: The computed score.
#     """


#     # format_score
#     is_format_error, error_messages = check_model_format(solution_str)
#     has_tool_usage = bool(
#         re.search(r"<tool_call>.*?</tool_call>", solution_str, re.DOTALL)
#         or re.search(r"<tool_response>.*?</tool_response>", solution_str, re.DOTALL)
#     )
#     if has_tool_usage:
#         is_call_tool = True
#     else:
#         is_call_tool = False
#     # accuracy_score
#     answer_text = solution_str.split("<answer>")[-1].split("</answer>")[0].strip()
#     prompt = get_prompt(ground_truth, answer_text)
#     chat_response = CLIENT.chat.completions.create(
#         model=MODEL_NAME,
#         messages=[
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©çš„è¯„åˆ†åŠ©æ‰‹ï¼Œè´Ÿè´£æ ¹æ®æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬è¿›è¡Œè¯„åˆ†ã€‚"},
#             {"role": "user", "content": prompt},
#         ],
#         seed = random.randint(0, 1000000),
#         temperature=0.3,
#     )
#     response = chat_response.choices[0].message.content.strip()
#     result = extract_fields(response)
#     dct_score = {
#         "ç»“æ„": result.get("ç»“æ„", None),
#         "æœä»£": result.get("æœä»£", None),
#         "çš‡å¸": result.get("çš‡å¸", None),
#         "é‡‰è‰²": result.get("é‡‰è‰²", None),
#         "çº¹é¥°": result.get("çº¹é¥°", None),
#         "å™¨å‹": result.get("å™¨å‹", None),
#     }
#     acc_reward = 0
#     n_scores = 0
#     for k, v in dct_score.items():
#         if  (v is None) or (v < 0):
#             continue
#         n_scores += 1
#         acc_reward += v
#     if n_scores > 0:
#         acc_reward = acc_reward/n_scores
#     else:
#         acc_reward = 0.0

#     format_reward = -1.0 if is_format_error else 0.0
#     tool_reward_1 = 1.0 if is_call_tool else 0.0
#     tool_reward_2 = acc_reward if is_call_tool  else 0.0

#     res = 0.8 * acc_reward + 0.2 * format_reward + 0.5 * tool_reward_1 + 0.5 * tool_reward_2
#     # Debugging output
#     logger.info(f'''
# [DEBUG]=========================================== 
# Ground Truth: {ground_truth},
# Model Response: {solution_str},
# Extracted JUDGE Result: {result},
# Final Computed Score: {res}, 
# acc_reward: {acc_reward}, 
# format_reward: {format_reward}, 
# tool_reward_1: {tool_reward_1},
# Format Error: {error_messages}
# ===========================================
# ''')
    


#     dct_return = {
#         "score": res,
#         "acc_reward": acc_reward,
#         "format_reward": format_reward,
#         "tool_reward_1": tool_reward_1,
#         "tool_reward_2": tool_reward_2,
#     }
#     return dct_return



def compute_score_tf(data_source: str, solution_str: str, ground_truth: str, extra_info=None) -> float:
    """
    Compute the score based on the predicted string and ground truth.

    In this version, the scoring is adjusted to give more weight to the tool usage and format correctness.
    
    Args:
        predict_str (str): The predicted string.
        ground_truth (str): The ground truth string.
        extra_info (dict, optional): Additional information, not used in this function.

    Returns:
        float: The computed score.
    """


    # format_score
    is_format_error, error_messages = check_model_format(solution_str)
    has_success_response = bool(
    re.search(r"<tool_response>.*?æˆåŠŸ.*?</tool_response>", solution_str, re.DOTALL)
    )

    if has_success_response:
        is_call_tool = True
    else:
        is_call_tool = False
    # accuracy_score
    answer_text = solution_str.split("<answer>")[-1].split("</answer>")[0].strip()
    acc_reward = 0.0
    if "çœŸ" in answer_text and "èµ" not in answer_text:
        if "çœŸ" in ground_truth:
            acc_reward = 1.0
    if "èµ" in answer_text and "çœŸ" not in answer_text:
        if "èµ" in ground_truth:
            acc_reward = 1.0
    # if answer_text == ground_truth:
    #     acc_reward = 1.0
    # else:
    #     acc_reward = 0.0

    format_reward = -1.0 if is_format_error else 0.0
    tool_reward_1 = 1.0 if is_call_tool else 0.0
    tool_reward_2 = acc_reward if is_call_tool  else 0.0

    res = 1.2 * acc_reward + 0.2 * format_reward + 0.5 * tool_reward_1 + 0.5 * tool_reward_2
    # Debugging output
    print(f'''
[DEBUG]=========================================== 
Ground Truth: {ground_truth},
Model Response: {solution_str},
Final Computed Score: {res}, 
acc_reward: {acc_reward}, 
format_reward: {format_reward}, 
tool_reward_1: {tool_reward_1},
Format Error: {error_messages}
===========================================
''')

    dct_return = {
        "score": res,
        "acc_reward": acc_reward,
        "format_reward": format_reward,
        "tool_reward_1": tool_reward_1,
        "tool_reward_2": tool_reward_2,
    }
    return dct_return

